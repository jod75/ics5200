{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 - ChEMBL data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Throughout this notebook, we will explore the ChEMBL database. We will look at the number of rows, the type of data available and the SQL statements needed to extract the required data.  We will do some cleaning too, if this is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "config = {\n",
    "  'user': 'joseph',\n",
    "  'password': 'password',\n",
    "  'host': '192.168.151.11',\n",
    "  'database': 'chembl_22',\n",
    "  'raise_on_warnings': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Molecules\n",
    "We need the following data\n",
    "- molregno: this is an integer used to identify a molecule\n",
    "- canonical_smiles: a varchar(4000) identifying the molecule in SMILES format\n",
    "\n",
    "NOTE: All data is filtered using Taxonomy ID = 9606 -> Homo Sapiens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n"
     ]
    }
   ],
   "source": [
    "# Get all molecules from ChEMBL and store them in a SPARK dataframe\n",
    "molecules = None   # a dataframe to store small molecules\n",
    "\n",
    "try:    \n",
    "    cnx = mysql.connector.connect(**config)\n",
    "  \n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    query = (\"SELECT DISTINCTROW cs.molregno, cs.canonical_smiles \" +\n",
    "             \"FROM compound_structures cs, activities act, \" +\n",
    "             \"     assays asy \" +\n",
    "             \"WHERE cs.molregno = act.molregno AND\" +\n",
    "             \"     act.assay_id = asy.assay_id AND\" +\n",
    "             \"     asy.assay_tax_id = 9606\")\n",
    "\n",
    "    cursor.execute(query)\n",
    "\n",
    "    tempMolecules = []\n",
    "    i = 0\n",
    "    for (molregno, canonical_smiles) in cursor:\n",
    "        i = i + 1\n",
    "        tempMolecules.append((molregno, canonical_smiles))\n",
    "        if i % 10000 == 0:\n",
    "            print i\n",
    "            if molecules is None:\n",
    "                molecules = spark.createDataFrame(tempMolecules)\n",
    "            else:\n",
    "                _molecules = spark.createDataFrame(tempMolecules)\n",
    "                molecules = molecules.union(_molecules)\n",
    "            tempMolecules = []\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "        print(\"Something is wrong with your user name or password\")\n",
    "    elif err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "        print(\"Database does not exist\")\n",
    "    else:\n",
    "        print(err)\n",
    "else:\n",
    "    cnx.close()\n",
    "    \n",
    "# store the rest  \n",
    "if len(tempMolecules) > 0:\n",
    "    _molecules = spark.createDataFrame(tempMolecules)\n",
    "    molecules = molecules.union(_molecules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 991054 molecules in hdfs\n"
     ]
    }
   ],
   "source": [
    "# store molecules in hadoop hdfs\n",
    "print(\"Storing %d molecules in hdfs\" % molecules.count())\n",
    "molecules.coalesce(1).write.csv(\"hdfs://hadoop1:9000/ics5200/molecules.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Target components\n",
    "Target proteins in ChEMBL are stored under the components group of tables.  Next, we will get the component ID and sequence for each component in ChEMBL.  Sequence is the FASTA sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "# Get all proteins from ChEMBL and store them in a SPARK dataframe\n",
    "proteins = None   # a dataframe to store proteins\n",
    "\n",
    "try:\n",
    "    cnx = mysql.connector.connect(**config)\n",
    "\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    query = (\"SELECT DISTINCTROW component_id, sequence \" +\n",
    "             \"FROM component_sequences \" +\n",
    "             \"WHERE tax_id = 9606\")\n",
    "\n",
    "    cursor.execute(query)\n",
    "\n",
    "    tempProteins = []\n",
    "    i = 0\n",
    "    for (component_id, sequence) in cursor:\n",
    "        i = i + 1\n",
    "        tempProteins.append((component_id, sequence))\n",
    "        if i % 1000 == 0:\n",
    "            print i\n",
    "            if proteins is None:\n",
    "                proteins = spark.createDataFrame(tempProteins)\n",
    "            else:\n",
    "                _proteins = spark.createDataFrame(tempProteins)\n",
    "                proteins = proteins.union(_proteins)\n",
    "            tempProteins = []\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "        print(\"Something is wrong with your user name or password\")\n",
    "    elif err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "        print(\"Database does not exist\")\n",
    "    else:\n",
    "        print(err)\n",
    "else:\n",
    "    cnx.close()\n",
    "\n",
    "# store the rest  \n",
    "if len(tempProteins) > 0:\n",
    "    _proteins = spark.createDataFrame(tempProteins)\n",
    "    proteins = proteins.union(_proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 3130 proteins in hdfs\n"
     ]
    }
   ],
   "source": [
    "# store proteins in hadoop hdfs\n",
    "print(\"Storing %d proteins in hdfs\" % proteins.count())\n",
    "proteins.coalesce(1).write.csv(\"hdfs://hadoop1:9000/ics5200/proteins.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ligand-Protein bindings\n",
    "Bindings in ChEMBL are extracted from activities and assays.  we store the following data:\n",
    "- ASSAY_ID: Unique ID for the assay\n",
    "- MOLREGNO: Unique compound (moleucule) ID\n",
    "- STANDARD_RELATION: Symbol constraining the activity value (e.g. >, <, =)\n",
    "- STANDARD_VALUE: Datapoint value from published source but transformed to common units: e.g. mM concentrations converted to nM.\n",
    "- STANDARD_UNITS: Selected 'Standard' units for data type: e.g. concentrations are in nM.\n",
    "- STANDARD_TYPE: Standardised version of the published_activity_type (e.g. IC50 rather than Ic-50/Ic50/ic50/ic-50)\n",
    "- PCHEMBL_VALUE: Negative log of selected concentration-response activity values (IC50/EC50/XC50/AC50/Ki/Kd/Potency)\n",
    "\n",
    "**NOTE:** We shall cast decimal values as in MYSQL Decimal values are stored as DECIMAL(64,30), which is not supported in Python.  Altough this will generate a number of 1264 Warings, this is not affecting our final result, as ChEMBL values are up to two significat figures (https://www.ebi.ac.uk/chembl/faq):\n",
    "\n",
    "In addition to the conversion of published activity types/values/units to standard activity types/values/units, described in previous releases, a number of further enhancements have been made to the data in the activities table: \n",
    "\n",
    "Conversion of log/-log values to nM concentrations. For example pIC50 and log Ki have been converted to IC50 and Ki values.\n",
    "Rounding of standard values to three significant figures (or 2 decimal places for values > 10)\n",
    "Flagging of data with possible errors (using the data_validity_comment field), such as unusual units for the activity type, or very large/small numbers.\n",
    "Identification of potential duplicate values - where an activity measurement is likely to be a repeat citation of an earlier measurement, rather than an independent measurement (flagged using the potential_duplicate column).\n",
    "\n",
    "An additional table (ACTIVITY_STDS_LOOKUP) has been included in this release, which contains details of the standard_types that have been standardised, their permitted standard_units, and acceptable value ranges.\n",
    "\n",
    "``+---------+------+-------------------------------------------------------------------------------------+\n",
    "| Level   | Code | Message                                                                             |\n",
    "+---------+------+-------------------------------------------------------------------------------------+\n",
    "| Warning | 1264 | Out of range value for column 'CAST(act.standard_value as DECIMAL(38,30))' at row 1 |\n",
    "| Warning | 1264 | Out of range value for column 'CAST(act.standard_value as DECIMAL(38,30))' at row 1 |\n",
    "|....                                                                                                  |\n",
    "+---------+------+-------------------------------------------------------------------------------------+``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "There were Data out of range warnings, but this is ok.  See note in Jupyter Notebook.\n"
     ]
    }
   ],
   "source": [
    "# Get all proteins from ChEMBL and store them in a SPARK dataframe\n",
    "bindings = None   # a dataframe to store proteins\n",
    "\n",
    "try:\n",
    "    # some data is null, thus Spark cannot infer the schema\n",
    "    # assay_id, molregno, std_relation, std_value, std_units, std_type, pchembl_value, component_id\n",
    "    # this schema is infered after anlysing the schema using rdd.printSchema()\n",
    "    from pyspark.sql.types import *\n",
    "    bindingSchema = StructType([StructField(\"assay_id\", LongType(), True),\n",
    "                                StructField(\"molregno\", LongType(), True),\n",
    "                                StructField(\"std_relation\", StringType(), True),\n",
    "                                StructField(\"std_value\", DecimalType(38,30), True),\n",
    "                                StructField(\"std_units\", StringType(), True),\n",
    "                                StructField(\"std_type\", StringType(), True),\n",
    "                                StructField(\"pchembl_value\", DecimalType(38,30), True),\n",
    "                                StructField(\"component_id\", LongType(), True)])\n",
    "\n",
    "    cnx = mysql.connector.connect(**config)\n",
    "\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    query = (\"SELECT DISTINCTROW act.assay_id, act.molregno, \" +\n",
    "             \"     act.standard_relation, CAST(act.standard_value as DECIMAL(38,30)), \" +\n",
    "             \"     act.standard_units, act.standard_type, \" +\n",
    "             \"     CAST(act.pchembl_value as DECIMAL(38,30)), tc.component_id \" +\n",
    "             \"FROM activities act, assays asy,  \" +\n",
    "             \"     target_components tc \"\n",
    "             \"WHERE act.assay_id = asy.assay_id AND \" +\n",
    "             \"      asy.tid = tc.tid AND \"+ \n",
    "             \"      asy.assay_tax_id = 9606\")\n",
    "\n",
    "    cursor.execute(query)\n",
    "\n",
    "    tempBindings = []\n",
    "    i = 0\n",
    "    for (assay_id, molregno, std_relation, std_value, std_units, std_type, pchembl_value, component_id) in cursor:\n",
    "        i = i + 1\n",
    "        tempBindings.append((assay_id, molregno, std_relation, std_value, std_units, std_type, pchembl_value, component_id))\n",
    "        if i % 100000 == 0:\n",
    "            print i\n",
    "            if bindings is None:\n",
    "                bindings = spark.createDataFrame(tempBindings, bindingSchema)\n",
    "            else:\n",
    "                _bindings = spark.createDataFrame(tempBindings, bindingSchema)\n",
    "                bindings = bindings.union(_bindings)\n",
    "            tempBindings = []\n",
    "    \n",
    "except mysql.connector.Error as err:\n",
    "    if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "        print(\"Something is wrong with your user name or password\")\n",
    "    elif err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "        print(\"Database does not exist\")\n",
    "    elif err.errno == errorcode.ER_WARN_DATA_OUT_OF_RANGE:\n",
    "        print(\"There were Data out of range warnings, but this is ok.  See note in Jupyter Notebook.\")\n",
    "    else:\n",
    "        print(err)\n",
    "else:\n",
    "    cnx.close()\n",
    "\n",
    "# store the rest  \n",
    "if len(tempBindings) > 0:\n",
    "    _bindings = spark.createDataFrame(tempBindings, bindingSchema)\n",
    "    bindings = bindings.union(_bindings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### bindings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store brindings in hadoop hdfs\n",
    "#print(\"Storing %d bindings in hdfs\" % bindings.count())\n",
    "bindings.write.csv(\"hdfs://hadoop1:9000/ics5200/bindings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
